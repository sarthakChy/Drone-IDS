{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acda193",
   "metadata": {
    "id": "7acda193",
    "outputId": "270aaa80-4a2a-480d-e01b-232a2f58f00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Loading dataset...\n",
      "ðŸ“¦ Creating sliding windows...\n",
      "Starting window creation...\n",
      "Window creation finished.\n",
      "Total windows: 7282 | Features per window: 136\n",
      "Attack windows: 2643 | Normal windows: 4639\n",
      "Attack ratio: 0.363\n",
      "Calculated XGB scale_pos_weight: 1.76\n",
      "\n",
      "ðŸ§ª Setting up Stratified GroupKFold...\n",
      "Total unique groups (flights): 24\n",
      "Attack flights: 10 | Normal flights: 14\n",
      "\n",
      "--- Training and Evaluating: RandomForest ---\n",
      "Fold 1: Acc: 0.7706 | F1: 0.6362 | AUC: 0.7860\n",
      "Fold 2: Acc: 0.7927 | F1: 0.6209 | AUC: 0.8230\n",
      "Fold 3: Acc: 0.8095 | F1: 0.6471 | AUC: 0.8480\n",
      "Fold 4: Acc: 0.8457 | F1: 0.7537 | AUC: 0.8873\n",
      "Fold 5: Acc: 0.7926 | F1: 0.8032 | AUC: 0.9075\n",
      "--- RandomForest Mean: Acc: 0.8022 | F1: 0.6922 | AUC: 0.8503 ---\n",
      "\n",
      "--- Training and Evaluating: ExtraTrees ---\n",
      "Fold 1: Acc: 0.7493 | F1: 0.4938 | AUC: 0.7854\n",
      "Fold 2: Acc: 0.8499 | F1: 0.6471 | AUC: 0.8531\n",
      "Fold 3: Acc: 0.7653 | F1: 0.4067 | AUC: 0.8520\n",
      "Fold 4: Acc: 0.7937 | F1: 0.5687 | AUC: 0.8709\n",
      "Fold 5: Acc: 0.7411 | F1: 0.7188 | AUC: 0.9234\n",
      "--- ExtraTrees Mean: Acc: 0.7799 | F1: 0.5670 | AUC: 0.8570 ---\n",
      "\n",
      "--- Training and Evaluating: LogisticRegression ---\n",
      "Fold 1: Acc: 0.7012 | F1: 0.5435 | AUC: 0.7251\n",
      "Fold 2: Acc: 0.7921 | F1: 0.5800 | AUC: 0.7685\n",
      "Fold 3: Acc: 0.6849 | F1: 0.3919 | AUC: 0.5791\n",
      "Fold 4: Acc: 0.7059 | F1: 0.5465 | AUC: 0.6970\n",
      "Fold 5: Acc: 0.7600 | F1: 0.7905 | AUC: 0.8590\n",
      "--- LogisticRegression Mean: Acc: 0.7288 | F1: 0.5705 | AUC: 0.7257 ---\n",
      "\n",
      "--- Training and Evaluating: SVC-RBF ---\n",
      "Fold 1: Acc: 0.7287 | F1: 0.5358 | AUC: 0.7337\n",
      "Fold 2: Acc: 0.8273 | F1: 0.6102 | AUC: 0.8093\n",
      "Fold 3: Acc: 0.7423 | F1: 0.4470 | AUC: 0.7067\n",
      "Fold 4: Acc: 0.7746 | F1: 0.5964 | AUC: 0.7723\n",
      "Fold 5: Acc: 0.7926 | F1: 0.8087 | AUC: 0.8492\n",
      "--- SVC-RBF Mean: Acc: 0.7731 | F1: 0.5996 | AUC: 0.7743 ---\n",
      "\n",
      "--- Training and Evaluating: GradientBoosting ---\n",
      "Fold 1: Acc: 0.7576 | F1: 0.6073 | AUC: 0.7754\n",
      "Fold 2: Acc: 0.7772 | F1: 0.5835 | AUC: 0.8090\n",
      "Fold 3: Acc: 0.7871 | F1: 0.5896 | AUC: 0.8416\n",
      "Fold 4: Acc: 0.8113 | F1: 0.6969 | AUC: 0.8815\n",
      "Fold 5: Acc: 0.7093 | F1: 0.6962 | AUC: 0.8875\n",
      "--- GradientBoosting Mean: Acc: 0.7685 | F1: 0.6347 | AUC: 0.8390 ---\n",
      "\n",
      "--- Training and Evaluating: XGBoost ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/drone_project/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:32:35] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Acc: 0.7335 | F1: 0.5890 | AUC: 0.7616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/drone_project/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:32:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Acc: 0.6921 | F1: 0.4118 | AUC: 0.7563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/drone_project/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:32:39] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Acc: 0.7838 | F1: 0.6356 | AUC: 0.8353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/drone_project/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:32:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Acc: 0.8182 | F1: 0.7220 | AUC: 0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/drone_project/.venv/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:32:43] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Acc: 0.7578 | F1: 0.7674 | AUC: 0.8824\n",
      "--- XGBoost Mean: Acc: 0.7571 | F1: 0.6252 | AUC: 0.8234 ---\n",
      "\n",
      "==============================\n",
      "  Final Window-Based Model Comparison\n",
      "==============================\n",
      "                Model  Accuracy        F1       AUC\n",
      "1          ExtraTrees  0.779878  0.566985  0.856961\n",
      "0        RandomForest  0.802219  0.692206  0.850348\n",
      "4    GradientBoosting  0.768500  0.634715  0.839018\n",
      "5             XGBoost  0.757064  0.625155  0.823374\n",
      "3             SVC-RBF  0.773092  0.599623  0.774273\n",
      "2  LogisticRegression  0.728838  0.570487  0.725715\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "from collections import deque\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- Constants (from your script) ---\n",
    "WINDOW_SIZE = 50\n",
    "WINDOW_STRIDE = 5\n",
    "DATA_PATH = \"my_master_dataset_full.csv\"\n",
    "FEATURE_COLS = [\n",
    "    'att_roll', 'att_pitch', 'att_yaw',\n",
    "    'pos_lat', 'pos_lon', 'pos_alt_rel',\n",
    "    'pos_vx', 'pos_vy', 'pos_vz',\n",
    "    'nav_roll', 'nav_pitch', 'nav_alt_error',\n",
    "    'sys_voltage_battery', 'sys_load',\n",
    "    'vib_x', 'vib_y', 'vib_z'\n",
    "]\n",
    "\n",
    "def extract_window_features_safe(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Extract robust window-level features, safely handling NaNs.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        for col in df.columns:\n",
    "            col_data = df[col].astype(float).values\n",
    "\n",
    "            if np.all(np.isnan(col_data)):\n",
    "                features[f\"{col}_mean\"] = np.nan\n",
    "                features[f\"{col}_std\"] = np.nan\n",
    "                features[f\"{col}_min\"] = np.nan\n",
    "                features[f\"{col}_max\"] = np.nan\n",
    "                features[f\"{col}_slope\"] = np.nan\n",
    "                features[f\"{col}_range\"] = np.nan\n",
    "                features[f\"{col}_diff_mean\"] = np.nan\n",
    "                features[f\"{col}_diff_std\"] = np.nan\n",
    "                continue\n",
    "\n",
    "            features[f\"{col}_mean\"] = np.nanmean(col_data)\n",
    "            features[f\"{col}_std\"] = np.nanstd(col_data)\n",
    "            features[f\"{col}_min\"] = np.nanmin(col_data)\n",
    "            features[f\"{col}_max\"] = np.nanmax(col_data)\n",
    "\n",
    "            try:\n",
    "                valid_indices = np.where(~np.isnan(col_data))[0]\n",
    "                first_valid_idx = valid_indices[0]\n",
    "                last_valid_idx = valid_indices[-1]\n",
    "                features[f\"{col}_slope\"] = col_data[last_valid_idx] - col_data[first_valid_idx]\n",
    "            except IndexError:\n",
    "                features[f\"{col}_slope\"] = np.nan\n",
    "\n",
    "            features[f\"{col}_range\"] = np.nanmax(col_data) - np.nanmin(col_data)\n",
    "            diff = np.diff(col_data)\n",
    "\n",
    "            if len(diff[~np.isnan(diff)]) > 0:\n",
    "                features[f\"{col}_diff_mean\"] = np.nanmean(diff)\n",
    "                features[f\"{col}_diff_std\"] = np.nanstd(diff)\n",
    "            else:\n",
    "                features[f\"{col}_diff_mean\"] = np.nan\n",
    "                features[f\"{col}_diff_std\"] = np.nan\n",
    "    return features\n",
    "\n",
    "# --- create_windows function (unchanged) ---\n",
    "def create_windows(df):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    groups = []\n",
    "    print(\"Starting window creation...\")\n",
    "    for fid, g in df.groupby(\"flight_id\"):\n",
    "        arr = g[FEATURE_COLS].values\n",
    "        lbl = g[\"label\"].values\n",
    "        n = len(arr)\n",
    "\n",
    "        if n < WINDOW_SIZE:\n",
    "            continue\n",
    "\n",
    "        for start in range(0, n - WINDOW_SIZE + 1, WINDOW_STRIDE):\n",
    "            w = arr[start:start+WINDOW_SIZE]\n",
    "            wdf = pd.DataFrame(w, columns=FEATURE_COLS).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "            feats = extract_window_features_safe(wdf)\n",
    "            windows.append(feats)\n",
    "            labels.append(int(np.any(lbl[start:start+WINDOW_SIZE] == 1)))\n",
    "            groups.append(fid)\n",
    "\n",
    "    print(\"Window creation finished.\")\n",
    "    return pd.DataFrame(windows), np.array(labels), np.array(groups)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸ“˜ Loading dataset...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df = df.dropna(subset=FEATURE_COLS)\n",
    "\n",
    "    print(\"ðŸ“¦ Creating sliding windows...\")\n",
    "    X, y, groups = create_windows(df)\n",
    "\n",
    "    print(f\"Total windows: {len(X)} | Features per window: {X.shape[1]}\")\n",
    "    print(f\"Attack windows: {sum(y)} | Normal windows: {len(y) - sum(y)}\")\n",
    "    print(f\"Attack ratio: {sum(y) / len(y):.3f}\")\n",
    "\n",
    "    # --- Define Models ---\n",
    "    # Calculate scale_pos_weight for XGBoost\n",
    "    try:\n",
    "        count_neg = (y == 0).sum()\n",
    "        count_pos = (y == 1).sum()\n",
    "        scale_pos_weight = count_neg / count_pos\n",
    "        print(f\"Calculated XGB scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "    except ZeroDivisionError:\n",
    "        scale_pos_weight = 1\n",
    "        print(\"Warning: No positive samples (label=1) found.\")\n",
    "\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=300, max_depth=14, class_weight='balanced', n_jobs=-1, random_state=42),\n",
    "        \"ExtraTrees\": ExtraTreesClassifier(n_estimators=200, max_depth=12, n_jobs=-1, random_state=42),\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=500, class_weight='balanced', n_jobs=-1),\n",
    "        \"SVC-RBF\": SVC(kernel=\"rbf\", probability=True, class_weight='balanced'),\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            n_estimators=200, learning_rate=0.05, max_depth=8, n_jobs=-1,\n",
    "            use_label_encoder=False, eval_metric='logloss',\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # --- Stratified GroupKFold Setup ---\n",
    "    print(\"\\nðŸ§ª Setting up Stratified GroupKFold...\")\n",
    "    group_df = pd.DataFrame({'group': groups, 'label': y})\n",
    "    group_labels_df = group_df.groupby('group')['label'].max()\n",
    "    unique_groups = group_labels_df.index.values\n",
    "    group_labels = group_labels_df.values\n",
    "\n",
    "    print(f\"Total unique groups (flights): {len(unique_groups)}\")\n",
    "    print(f\"Attack flights: {sum(group_labels)} | Normal flights: {len(group_labels) - sum(group_labels)}\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    # --- Run Model Comparison ---\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training and Evaluating: {name} ---\")\n",
    "        fold_metrics = []\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "\n",
    "        for fold, (train_group_indices, test_group_indices) in enumerate(skf.split(unique_groups, group_labels)):\n",
    "\n",
    "            train_groups = unique_groups[train_group_indices]\n",
    "            test_groups = unique_groups[test_group_indices]\n",
    "\n",
    "            train_idx = np.where(np.isin(groups, train_groups))[0]\n",
    "            test_idx = np.where(np.isin(groups, test_groups))[0]\n",
    "\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            if len(np.unique(y_test)) < 2:\n",
    "                print(f\"Fold {fold+1}: WARNING: Test set has only one class. Skipping fold.\")\n",
    "                continue\n",
    "\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_pred = pipe.predict(X_test)\n",
    "\n",
    "            y_prob = (\n",
    "                pipe.predict_proba(X_test)[:, 1]\n",
    "                if hasattr(pipe.named_steps[\"model\"], \"predict_proba\")\n",
    "                else y_pred\n",
    "            )\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_prob)\n",
    "            fold_metrics.append((acc, f1, auc))\n",
    "            print(f\"Fold {fold+1}: Acc: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "        if not fold_metrics:\n",
    "            print(f\"Model {name} failed to train (all folds skipped).\")\n",
    "            continue\n",
    "\n",
    "        acc_mean = np.mean([m[0] for m in fold_metrics])\n",
    "        f1_mean = np.mean([m[1] for m in fold_metrics])\n",
    "        auc_mean = np.nanmean([m[2] for m in fold_metrics])\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": acc_mean,\n",
    "            \"F1\": f1_mean,\n",
    "            \"AUC\": auc_mean\n",
    "        })\n",
    "        print(f\"--- {name} Mean: Acc: {acc_mean:.4f} | F1: {f1_mean:.4f} | AUC: {auc_mean:.4f} ---\")\n",
    "\n",
    "    # --- Print Final Results ---\n",
    "    results_df = pd.DataFrame(results).sort_values(\"AUC\", ascending=False)\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"  Final Window-Based Model Comparison\")\n",
    "    print(\"=\"*30)\n",
    "    print(results_df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c126b0d0",
   "metadata": {
    "id": "c126b0d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Loading dataset...\n",
      "ðŸ“¦ Creating sliding windows for training...\n",
      "Starting window creation...\n",
      "Window creation finished.\n",
      "Total windows: 7282 | Features per window: 136\n",
      "Attack windows: 2158 | Normal windows: 5124\n",
      "\n",
      "ðŸš€ Training final RandomForest model on ALL data...\n",
      "âœ… Model training complete.\n",
      "ðŸ’¾ Final model saved to: window_ids_model.pkl\n",
      "ðŸ’¾ Feature list saved to: window_feature_list.pkl\n",
      "ðŸ’¾ Window size saved to: WINDOW_SIZE.pkl\n",
      "\n",
      "ðŸŽ‰ All artifacts are ready for the dashboard!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- Constants ---\n",
    "WINDOW_SIZE = 50\n",
    "WINDOW_STRIDE = 5  # This is only for training; in real-time, the stride is 1\n",
    "DATA_PATH = \"my_master_dataset_RELABELED.csv\"\n",
    "FEATURE_COLS = [\n",
    "    'att_roll', 'att_pitch', 'att_yaw',\n",
    "    'pos_lat', 'pos_lon', 'pos_alt_rel',\n",
    "    'pos_vx', 'pos_vy', 'pos_vz',\n",
    "    'nav_roll', 'nav_pitch', 'nav_alt_error',\n",
    "    'sys_voltage_battery', 'sys_load',\n",
    "    'vib_x', 'vib_y', 'vib_z'\n",
    "]\n",
    "\n",
    "# --- Final Model Filenames ---\n",
    "MODEL_SAVE_PATH = \"window_ids_model.pkl\"\n",
    "FEATURE_LIST_PATH = \"window_feature_list.pkl\"\n",
    "WINDOW_SIZE_PATH = \"WINDOW_SIZE.pkl\"\n",
    "\n",
    "# --- NAN-SAFE FEATURE EXTRACTOR ---\n",
    "# This must be the same as 'window_feature.py'\n",
    "def extract_window_features_safe(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Extract robust window-level features, safely handling NaNs.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        for col in df.columns:\n",
    "            col_data = df[col].astype(float).values\n",
    "\n",
    "            if np.all(np.isnan(col_data)):\n",
    "                features[f\"{col}_mean\"] = np.nan\n",
    "                features[f\"{col}_std\"] = np.nan\n",
    "                features[f\"{col}_min\"] = np.nan\n",
    "                features[f\"{col}_max\"] = np.nan\n",
    "                features[f\"{col}_slope\"] = np.nan\n",
    "                features[f\"{col}_range\"] = np.nan\n",
    "                features[f\"{col}_diff_mean\"] = np.nan\n",
    "                features[f\"{col}_diff_std\"] = np.nan\n",
    "                continue\n",
    "\n",
    "            features[f\"{col}_mean\"] = np.nanmean(col_data)\n",
    "            features[f\"{col}_std\"] = np.nanstd(col_data)\n",
    "            features[f\"{col}_min\"] = np.nanmin(col_data)\n",
    "            features[f\"{col}_max\"] = np.nanmax(col_data)\n",
    "\n",
    "            try:\n",
    "                valid_indices = np.where(~np.isnan(col_data))[0]\n",
    "                first_valid_idx = valid_indices[0]\n",
    "                last_valid_idx = valid_indices[-1]\n",
    "                features[f\"{col}_slope\"] = col_data[last_valid_idx] - col_data[first_valid_idx]\n",
    "            except IndexError:\n",
    "                features[f\"{col}_slope\"] = np.nan\n",
    "\n",
    "            features[f\"{col}_range\"] = np.nanmax(col_data) - np.nanmin(col_data)\n",
    "            diff = np.diff(col_data)\n",
    "\n",
    "            if len(diff[~np.isnan(diff)]) > 0:\n",
    "                features[f\"{col}_diff_mean\"] = np.nanmean(diff)\n",
    "                features[f\"{col}_diff_std\"] = np.nanstd(diff)\n",
    "            else:\n",
    "                features[f\"{col}_diff_mean\"] = np.nan\n",
    "                features[f\"{col}_diff_std\"] = np.nan\n",
    "    return features\n",
    "\n",
    "# --- create_windows function ---\n",
    "def create_windows(df):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    groups = []\n",
    "    print(\"Starting window creation...\")\n",
    "    for fid, g in df.groupby(\"flight_id\"):\n",
    "        arr = g[FEATURE_COLS].values\n",
    "        lbl = g[\"label\"].values\n",
    "        n = len(arr)\n",
    "\n",
    "        if n < WINDOW_SIZE:\n",
    "            continue\n",
    "\n",
    "        for start in range(0, n - WINDOW_SIZE + 1, WINDOW_STRIDE):\n",
    "            w = arr[start:start+WINDOW_SIZE]\n",
    "            wdf = pd.DataFrame(w, columns=FEATURE_COLS).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "            feats = extract_window_features_safe(wdf)\n",
    "            windows.append(feats)\n",
    "            labels.append(int(np.any(lbl[start:start+WINDOW_SIZE] == 1)))\n",
    "            groups.append(fid) # We don't use groups, but good to keep\n",
    "\n",
    "    print(\"Window creation finished.\")\n",
    "    return pd.DataFrame(windows), np.array(labels), np.array(groups)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸ“˜ Loading dataset...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df = df.dropna(subset=FEATURE_COLS)\n",
    "\n",
    "    print(\"ðŸ“¦ Creating sliding windows for training...\")\n",
    "    X, y, groups = create_windows(df)\n",
    "\n",
    "    print(f\"Total windows: {len(X)} | Features per window: {X.shape[1]}\")\n",
    "    print(f\"Attack windows: {sum(y)} | Normal windows: {len(y) - sum(y)}\")\n",
    "\n",
    "    # Save the exact feature list the model will be trained on\n",
    "    feature_list = list(X.columns)\n",
    "\n",
    "    # --- Define Our Winning Model ---\n",
    "    # RandomForestClassifier with parameters from the comparison script\n",
    "    final_model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=14,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # --- Build Final Pipeline ---\n",
    "    pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), # Handles any NaNs from feature extraction\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", final_model)\n",
    "    ])\n",
    "\n",
    "    print(\"\\nðŸš€ Training final RandomForest model on ALL data...\")\n",
    "    # Train the pipeline on the entire dataset\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    print(\"âœ… Model training complete.\")\n",
    "\n",
    "    # --- Save Artifacts for Dashboard ---\n",
    "    joblib.dump(pipeline, MODEL_SAVE_PATH)\n",
    "    print(f\"ðŸ’¾ Final model saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    joblib.dump(feature_list, FEATURE_LIST_PATH)\n",
    "    print(f\"ðŸ’¾ Feature list saved to: {FEATURE_LIST_PATH}\")\n",
    "\n",
    "    joblib.dump(WINDOW_SIZE, WINDOW_SIZE_PATH)\n",
    "    print(f\"ðŸ’¾ Window size saved to: {WINDOW_SIZE_PATH}\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ All artifacts are ready for the dashboard!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b000f5e8-ca57-4db9-8ab9-c599d6062218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/sarthak/anaconda3/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /home/sarthak/anaconda3/lib/python3.12/site-packages (from xgboost) (1.26.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/sarthak/anaconda3/lib/python3.12/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in /home/sarthak/anaconda3/lib/python3.12/site-packages (from xgboost) (1.13.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/home/sarthak/anaconda3/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f069db-2ac3-43b2-bbe3-dbb210d6fcd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# --- Constants (from your script) ---\u001b[39;00m\n\u001b[1;32m     18\u001b[0m WINDOW_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "from collections import deque\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- Constants (from your script) ---\n",
    "WINDOW_SIZE = 50\n",
    "WINDOW_STRIDE = 5\n",
    "DATA_PATH = \"my_master_dataset_RELABELED.csv\"\n",
    "FEATURE_COLS = [\n",
    "    'att_roll', 'att_pitch', 'att_yaw',\n",
    "    'pos_lat', 'pos_lon', 'pos_alt_rel',\n",
    "    'pos_vx', 'pos_vy', 'pos_vz',\n",
    "    'nav_roll', 'nav_pitch', 'nav_alt_error',\n",
    "    'sys_voltage_battery', 'sys_load',\n",
    "    'vib_x', 'vib_y', 'vib_z'\n",
    "]\n",
    "\n",
    "def extract_window_features_safe(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Extract robust window-level features, safely handling NaNs.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        for col in df.columns:\n",
    "            col_data = df[col].astype(float).values\n",
    "\n",
    "            if np.all(np.isnan(col_data)):\n",
    "                features[f\"{col}_mean\"] = np.nan\n",
    "                features[f\"{col}_std\"] = np.nan\n",
    "                features[f\"{col}_min\"] = np.nan\n",
    "                features[f\"{col}_max\"] = np.nan\n",
    "                features[f\"{col}_slope\"] = np.nan\n",
    "                features[f\"{col}_range\"] = np.nan\n",
    "                features[f\"{col}_diff_mean\"] = np.nan\n",
    "                features[f\"{col}_diff_std\"] = np.nan\n",
    "                continue\n",
    "\n",
    "            features[f\"{col}_mean\"] = np.nanmean(col_data)\n",
    "            features[f\"{col}_std\"] = np.nanstd(col_data)\n",
    "            features[f\"{col}_min\"] = np.nanmin(col_data)\n",
    "            features[f\"{col}_max\"] = np.nanmax(col_data)\n",
    "\n",
    "            try:\n",
    "                valid_indices = np.where(~np.isnan(col_data))[0]\n",
    "                first_valid_idx = valid_indices[0]\n",
    "                last_valid_idx = valid_indices[-1]\n",
    "                features[f\"{col}_slope\"] = col_data[last_valid_idx] - col_data[first_valid_idx]\n",
    "            except IndexError:\n",
    "                features[f\"{col}_slope\"] = np.nan\n",
    "\n",
    "            features[f\"{col}_range\"] = np.nanmax(col_data) - np.nanmin(col_data)\n",
    "            diff = np.diff(col_data)\n",
    "\n",
    "            if len(diff[~np.isnan(diff)]) > 0:\n",
    "                features[f\"{col}_diff_mean\"] = np.nanmean(diff)\n",
    "                features[f\"{col}_diff_std\"] = np.nanstd(diff)\n",
    "            else:\n",
    "                features[f\"{col}_diff_mean\"] = np.nan\n",
    "                features[f\"{col}_diff_std\"] = np.nan\n",
    "    return features\n",
    "\n",
    "# --- create_windows function (unchanged) ---\n",
    "def create_windows(df):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    groups = []\n",
    "    print(\"Starting window creation...\")\n",
    "    for fid, g in df.groupby(\"flight_id\"):\n",
    "        arr = g[FEATURE_COLS].values\n",
    "        lbl = g[\"label\"].values\n",
    "        n = len(arr)\n",
    "\n",
    "        if n < WINDOW_SIZE:\n",
    "            continue\n",
    "\n",
    "        for start in range(0, n - WINDOW_SIZE + 1, WINDOW_STRIDE):\n",
    "            w = arr[start:start+WINDOW_SIZE]\n",
    "            wdf = pd.DataFrame(w, columns=FEATURE_COLS).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "            feats = extract_window_features_safe(wdf)\n",
    "            windows.append(feats)\n",
    "            labels.append(int(np.any(lbl[start:start+WINDOW_SIZE] == 1)))\n",
    "            groups.append(fid)\n",
    "\n",
    "    print(\"Window creation finished.\")\n",
    "    return pd.DataFrame(windows), np.array(labels), np.array(groups)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸ“˜ Loading dataset...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df = df.dropna(subset=FEATURE_COLS)\n",
    "\n",
    "    print(\"ðŸ“¦ Creating sliding windows...\")\n",
    "    X, y, groups = create_windows(df)\n",
    "\n",
    "    print(f\"Total windows: {len(X)} | Features per window: {X.shape[1]}\")\n",
    "    print(f\"Attack windows: {sum(y)} | Normal windows: {len(y) - sum(y)}\")\n",
    "    print(f\"Attack ratio: {sum(y) / len(y):.3f}\")\n",
    "\n",
    "    # --- Define Models ---\n",
    "    # Calculate scale_pos_weight for XGBoost\n",
    "    try:\n",
    "        count_neg = (y == 0).sum()\n",
    "        count_pos = (y == 1).sum()\n",
    "        scale_pos_weight = count_neg / count_pos\n",
    "        print(f\"Calculated XGB scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "    except ZeroDivisionError:\n",
    "        scale_pos_weight = 1\n",
    "        print(\"Warning: No positive samples (label=1) found.\")\n",
    "\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=300, max_depth=14, class_weight='balanced', n_jobs=-1, random_state=42),\n",
    "        \"ExtraTrees\": ExtraTreesClassifier(n_estimators=200, max_depth=12, n_jobs=-1, random_state=42),\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=500, class_weight='balanced', n_jobs=-1),\n",
    "        \"SVC-RBF\": SVC(kernel=\"rbf\", probability=True, class_weight='balanced'),\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            n_estimators=200, learning_rate=0.05, max_depth=8, n_jobs=-1,\n",
    "            use_label_encoder=False, eval_metric='logloss',\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # --- Stratified GroupKFold Setup ---\n",
    "    print(\"\\nðŸ§ª Setting up Stratified GroupKFold...\")\n",
    "    group_df = pd.DataFrame({'group': groups, 'label': y})\n",
    "    group_labels_df = group_df.groupby('group')['label'].max()\n",
    "    unique_groups = group_labels_df.index.values\n",
    "    group_labels = group_labels_df.values\n",
    "\n",
    "    print(f\"Total unique groups (flights): {len(unique_groups)}\")\n",
    "    print(f\"Attack flights: {sum(group_labels)} | Normal flights: {len(group_labels) - sum(group_labels)}\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    # --- Run Model Comparison ---\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training and Evaluating: {name} ---\")\n",
    "        fold_metrics = []\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "\n",
    "        for fold, (train_group_indices, test_group_indices) in enumerate(skf.split(unique_groups, group_labels)):\n",
    "\n",
    "            train_groups = unique_groups[train_group_indices]\n",
    "            test_groups = unique_groups[test_group_indices]\n",
    "\n",
    "            train_idx = np.where(np.isin(groups, train_groups))[0]\n",
    "            test_idx = np.where(np.isin(groups, test_groups))[0]\n",
    "\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            if len(np.unique(y_test)) < 2:\n",
    "                print(f\"Fold {fold+1}: WARNING: Test set has only one class. Skipping fold.\")\n",
    "                continue\n",
    "\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_pred = pipe.predict(X_test)\n",
    "\n",
    "            y_prob = (\n",
    "                pipe.predict_proba(X_test)[:, 1]\n",
    "                if hasattr(pipe.named_steps[\"model\"], \"predict_proba\")\n",
    "                else y_pred\n",
    "            )\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_prob)\n",
    "            fold_metrics.append((acc, f1, auc))\n",
    "            print(f\"Fold {fold+1}: Acc: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "        if not fold_metrics:\n",
    "            print(f\"Model {name} failed to train (all folds skipped).\")\n",
    "            continue\n",
    "\n",
    "        acc_mean = np.mean([m[0] for m in fold_metrics])\n",
    "        f1_mean = np.mean([m[1] for m in fold_metrics])\n",
    "        auc_mean = np.nanmean([m[2] for m in fold_metrics])\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": acc_mean,\n",
    "            \"F1\": f1_mean,\n",
    "            \"AUC\": auc_mean\n",
    "        })\n",
    "        print(f\"--- {name} Mean: Acc: {acc_mean:.4f} | F1: {f1_mean:.4f} | AUC: {auc_mean:.4f} ---\")\n",
    "\n",
    "    # --- Print Final Results ---\n",
    "    results_df = pd.DataFrame(results).sort_values(\"AUC\", ascending=False)\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"  Final Window-Based Model Comparison\")\n",
    "    print(\"=\"*30)\n",
    "    print(results_df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e555c-6737-4978-9b33-4ac1b33bb8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
